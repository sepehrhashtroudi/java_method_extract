01/26/2022 01:19:38 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=10, config_name='graphcodebert-base', dev_filename='dataset/evosuit/Evosuit_train.methods,dataset/evosuit/Evosuit_train.tests', do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=64, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='saved_models/dec_6_evosuit_graphcodebert_512_320/methods-tests//checkpoint-best-bleu/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=512, max_steps=-1, max_target_length=320, model_name_or_path='saved_models/dec_6_evosuit_graphcodebert_512_320/methods-tests//checkpoint-best-bleu/pytorch_model.bin', model_type='roberta', no_cuda=False, num_train_epochs=3, output_dir='saved_models/dec_6_evosuit_graphcodebert_512_320/methods-tests/', seed=42, source_lang='methods', test_filename='dataset/evosuit/Evosuit_train.methods,dataset/evosuit/Evosuit_train.tests', tokenizer_name='graphcodebert-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
Some weights of the model checkpoint at saved_models/dec_6_evosuit_graphcodebert_512_320/methods-tests//checkpoint-best-bleu/pytorch_model.bin were not used when initializing RobertaModel: ['encoder.encoder.layer.5.attention.output.dense.bias', 'encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.encoder.layer.4.intermediate.dense.weight', 'encoder.encoder.layer.5.attention.self.query.weight', 'encoder.encoder.layer.4.attention.self.value.weight', 'encoder.encoder.layer.2.output.dense.weight', 'encoder.encoder.layer.6.output.LayerNorm.bias', 'encoder.encoder.layer.0.attention.output.dense.weight', 'encoder.encoder.layer.11.attention.self.value.bias', 'encoder.encoder.layer.0.output.dense.weight', 'encoder.encoder.layer.6.attention.output.LayerNorm.bias', 'dense.weight', 'encoder.encoder.layer.5.attention.self.key.weight', 'encoder.encoder.layer.0.output.LayerNorm.bias', 'decoder.layers.4.multihead_attn.in_proj_bias', 'encoder.encoder.layer.6.attention.self.query.bias', 'encoder.encoder.layer.3.intermediate.dense.weight', 'decoder.layers.1.linear1.weight', 'decoder.layers.0.norm1.weight', 'encoder.encoder.layer.9.attention.output.dense.weight', 'decoder.layers.5.multihead_attn.out_proj.weight', 'encoder.encoder.layer.8.output.LayerNorm.weight', 'encoder.encoder.layer.8.attention.self.query.bias', 'decoder.layers.2.multihead_attn.out_proj.bias', 'encoder.encoder.layer.4.intermediate.dense.bias', 'encoder.encoder.layer.5.output.dense.weight', 'encoder.encoder.layer.5.output.LayerNorm.bias', 'decoder.layers.5.norm3.weight', 'encoder.encoder.layer.1.attention.output.dense.weight', 'encoder.encoder.layer.4.attention.self.key.weight', 'encoder.encoder.layer.7.output.LayerNorm.weight', 'encoder.encoder.layer.7.attention.self.value.weight', 'encoder.encoder.layer.9.attention.self.query.bias', 'encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.encoder.layer.10.attention.self.value.bias', 'encoder.encoder.layer.3.attention.self.query.weight', 'encoder.encoder.layer.7.intermediate.dense.weight', 'encoder.encoder.layer.1.output.dense.bias', 'encoder.encoder.layer.7.attention.self.query.bias', 'decoder.layers.2.linear1.weight', 'decoder.layers.4.linear2.bias', 'decoder.layers.1.multihead_attn.in_proj_weight', 'decoder.layers.0.linear1.bias', 'encoder.encoder.layer.3.attention.self.key.bias', 'decoder.layers.1.multihead_attn.out_proj.bias', 'encoder.encoder.layer.11.output.dense.bias', 'encoder.encoder.layer.7.intermediate.dense.bias', 'encoder.embeddings.LayerNorm.weight', 'encoder.encoder.layer.11.intermediate.dense.bias', 'encoder.encoder.layer.1.intermediate.dense.bias', 'decoder.layers.0.multihead_attn.in_proj_weight', 'encoder.encoder.layer.7.attention.self.key.bias', 'encoder.encoder.layer.2.attention.self.value.bias', 'lm_head.weight', 'encoder.encoder.layer.8.output.LayerNorm.bias', 'decoder.layers.4.linear1.bias', 'encoder.encoder.layer.9.output.dense.bias', 'encoder.encoder.layer.5.attention.self.value.bias', 'decoder.layers.3.norm2.bias', 'decoder.layers.2.norm1.weight', 'decoder.layers.0.norm1.bias', 'decoder.layers.0.norm3.bias', 'encoder.encoder.layer.3.output.LayerNorm.bias', 'decoder.layers.1.norm1.weight', 'encoder.encoder.layer.9.attention.self.query.weight', 'encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'encoder.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.encoder.layer.1.intermediate.dense.weight', 'decoder.layers.4.multihead_attn.in_proj_weight', 'encoder.encoder.layer.9.attention.self.key.bias', 'decoder.layers.3.linear2.weight', 'encoder.encoder.layer.1.attention.self.key.bias', 'encoder.encoder.layer.9.output.LayerNorm.bias', 'encoder.encoder.layer.2.attention.self.key.bias', 'decoder.layers.0.norm2.weight', 'encoder.encoder.layer.10.intermediate.dense.weight', 'encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.encoder.layer.11.attention.self.key.weight', 'decoder.layers.4.norm3.bias', 'encoder.encoder.layer.6.output.LayerNorm.weight', 'decoder.layers.4.norm3.weight', 'encoder.encoder.layer.0.attention.self.key.weight', 'decoder.layers.2.linear2.bias', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.5.multihead_attn.in_proj_bias', 'decoder.layers.4.norm1.bias', 'encoder.encoder.layer.3.attention.self.query.bias', 'encoder.embeddings.position_ids', 'encoder.encoder.layer.10.output.dense.bias', 'decoder.layers.0.linear1.weight', 'encoder.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.encoder.layer.8.attention.output.dense.weight', 'encoder.encoder.layer.11.attention.self.query.weight', 'decoder.layers.3.norm1.weight', 'encoder.encoder.layer.0.output.dense.bias', 'encoder.encoder.layer.6.attention.self.value.bias', 'encoder.encoder.layer.2.intermediate.dense.bias', 'decoder.layers.2.norm3.weight', 'encoder.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.encoder.layer.8.output.dense.weight', 'encoder.encoder.layer.10.output.dense.weight', 'encoder.encoder.layer.2.attention.self.query.bias', 'encoder.encoder.layer.2.attention.self.value.weight', 'encoder.encoder.layer.0.attention.output.dense.bias', 'encoder.encoder.layer.6.attention.output.dense.weight', 'encoder.encoder.layer.8.attention.self.query.weight', 'decoder.layers.4.linear1.weight', 'encoder.encoder.layer.0.intermediate.dense.weight', 'encoder.pooler.dense.bias', 'decoder.layers.1.multihead_attn.out_proj.weight', 'encoder.encoder.layer.8.attention.self.key.weight', 'decoder.layers.2.norm2.bias', 'encoder.encoder.layer.2.attention.output.dense.weight', 'decoder.layers.2.self_attn.in_proj_bias', 'encoder.encoder.layer.3.attention.output.dense.bias', 'decoder.layers.2.self_attn.in_proj_weight', 'decoder.layers.2.linear1.bias', 'decoder.layers.2.norm2.weight', 'decoder.layers.0.norm3.weight', 'encoder.encoder.layer.1.output.dense.weight', 'encoder.encoder.layer.8.intermediate.dense.bias', 'decoder.layers.2.linear2.weight', 'encoder.encoder.layer.1.attention.self.value.weight', 'encoder.encoder.layer.1.attention.output.LayerNorm.bias', 'decoder.layers.1.linear1.bias', 'decoder.layers.3.self_attn.in_proj_bias', 'decoder.layers.0.multihead_attn.out_proj.bias', 'decoder.layers.1.self_attn.in_proj_weight', 'decoder.layers.3.self_attn.in_proj_weight', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.2.multihead_attn.out_proj.weight', 'encoder.embeddings.LayerNorm.bias', 'encoder.encoder.layer.9.intermediate.dense.bias', 'encoder.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.encoder.layer.1.attention.self.key.weight', 'decoder.layers.3.norm3.bias', 'encoder.encoder.layer.1.output.LayerNorm.weight', 'encoder.encoder.layer.0.attention.output.LayerNorm.bias', 'decoder.layers.3.norm1.bias', 'decoder.layers.1.linear2.weight', 'encoder.encoder.layer.2.output.dense.bias', 'encoder.encoder.layer.0.intermediate.dense.bias', 'encoder.encoder.layer.0.attention.self.value.weight', 'encoder.encoder.layer.3.attention.self.key.weight', 'encoder.encoder.layer.5.output.dense.bias', 'decoder.layers.1.linear2.bias', 'encoder.encoder.layer.2.attention.self.query.weight', 'decoder.layers.1.norm3.bias', 'decoder.layers.2.norm1.bias', 'encoder.encoder.layer.9.attention.self.key.weight', 'encoder.encoder.layer.7.attention.output.dense.bias', 'encoder.encoder.layer.7.attention.self.value.bias', 'decoder.layers.5.linear2.bias', 'decoder.layers.5.norm1.weight', 'decoder.layers.5.linear1.bias', 'encoder.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.encoder.layer.0.attention.self.query.bias', 'encoder.embeddings.token_type_embeddings.weight', 'encoder.encoder.layer.3.intermediate.dense.bias', 'encoder.encoder.layer.10.output.LayerNorm.bias', 'decoder.layers.5.self_attn.in_proj_weight', 'encoder.encoder.layer.4.attention.self.query.weight', 'encoder.encoder.layer.4.attention.self.query.bias', 'encoder.encoder.layer.10.attention.self.query.weight', 'decoder.layers.4.norm1.weight', 'encoder.encoder.layer.7.attention.self.query.weight', 'encoder.encoder.layer.4.output.LayerNorm.weight', 'encoder.embeddings.word_embeddings.weight', 'decoder.layers.4.self_attn.in_proj_weight', 'encoder.encoder.layer.5.attention.self.key.bias', 'encoder.encoder.layer.0.attention.self.key.bias', 'encoder.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.encoder.layer.4.output.dense.weight', 'encoder.encoder.layer.1.attention.self.query.bias', 'encoder.encoder.layer.6.output.dense.bias', 'encoder.encoder.layer.10.attention.self.key.bias', 'encoder.encoder.layer.7.output.LayerNorm.bias', 'decoder.layers.3.linear2.bias', 'encoder.encoder.layer.4.attention.output.dense.bias', 'encoder.encoder.layer.2.intermediate.dense.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'encoder.encoder.layer.10.attention.self.query.bias', 'decoder.layers.4.multihead_attn.out_proj.weight', 'decoder.layers.5.norm1.bias', 'encoder.encoder.layer.10.output.LayerNorm.weight', 'decoder.layers.1.norm2.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'encoder.encoder.layer.6.output.dense.weight', 'encoder.encoder.layer.9.attention.self.value.weight', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.3.norm3.weight', 'decoder.layers.5.multihead_attn.out_proj.bias', 'decoder.layers.0.linear2.bias', 'decoder.layers.3.multihead_attn.in_proj_bias', 'encoder.encoder.layer.11.attention.output.LayerNorm.weight', 'decoder.layers.0.multihead_attn.out_proj.weight', 'decoder.layers.1.norm3.weight', 'decoder.layers.3.linear1.bias', 'encoder.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.encoder.layer.6.attention.output.LayerNorm.weight', 'decoder.layers.5.norm2.bias', 'encoder.encoder.layer.4.attention.output.dense.weight', 'encoder.encoder.layer.11.attention.self.query.bias', 'encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.encoder.layer.5.intermediate.dense.bias', 'decoder.layers.1.norm2.bias', 'encoder.encoder.layer.11.attention.self.key.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.4.multihead_attn.out_proj.bias', 'encoder.encoder.layer.1.output.LayerNorm.bias', 'decoder.layers.1.multihead_attn.in_proj_bias', 'encoder.encoder.layer.6.attention.self.value.weight', 'encoder.encoder.layer.10.attention.output.dense.bias', 'decoder.layers.0.norm2.bias', 'encoder.encoder.layer.8.attention.output.dense.bias', 'encoder.encoder.layer.9.attention.self.value.bias', 'encoder.encoder.layer.11.attention.output.dense.weight', 'decoder.layers.0.multihead_attn.in_proj_bias', 'decoder.layers.5.norm3.bias', 'encoder.embeddings.position_embeddings.weight', 'encoder.encoder.layer.5.output.LayerNorm.weight', 'decoder.layers.3.multihead_attn.out_proj.bias', 'encoder.encoder.layer.9.intermediate.dense.weight', 'encoder.encoder.layer.2.attention.output.dense.bias', 'encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'decoder.layers.3.multihead_attn.in_proj_weight', 'encoder.encoder.layer.7.output.dense.bias', 'encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.encoder.layer.4.output.LayerNorm.bias', 'decoder.layers.2.norm3.bias', 'encoder.encoder.layer.9.output.dense.weight', 'encoder.encoder.layer.8.output.dense.bias', 'encoder.encoder.layer.11.output.dense.weight', 'decoder.layers.5.linear1.weight', 'encoder.encoder.layer.3.output.LayerNorm.weight', 'encoder.encoder.layer.3.attention.self.value.weight', 'encoder.encoder.layer.8.attention.self.value.bias', 'decoder.layers.3.linear1.weight', 'bias', 'encoder.encoder.layer.2.output.LayerNorm.bias', 'encoder.encoder.layer.10.attention.self.value.weight', 'encoder.encoder.layer.1.attention.self.query.weight', 'encoder.encoder.layer.3.attention.self.value.bias', 'decoder.layers.5.self_attn.in_proj_bias', 'encoder.encoder.layer.4.output.dense.bias', 'encoder.encoder.layer.6.attention.output.dense.bias', 'decoder.layers.2.multihead_attn.in_proj_bias', 'decoder.layers.4.linear2.weight', 'encoder.encoder.layer.9.attention.output.dense.bias', 'encoder.encoder.layer.11.output.LayerNorm.bias', 'decoder.layers.1.self_attn.out_proj.bias', 'encoder.encoder.layer.8.attention.output.LayerNorm.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'encoder.encoder.layer.5.intermediate.dense.weight', 'encoder.encoder.layer.0.attention.self.value.bias', 'decoder.layers.4.self_attn.in_proj_bias', 'encoder.encoder.layer.10.attention.output.dense.weight', 'encoder.encoder.layer.11.attention.self.value.weight', 'encoder.encoder.layer.6.attention.self.key.bias', 'encoder.encoder.layer.11.intermediate.dense.weight', 'encoder.encoder.layer.9.output.LayerNorm.weight', 'decoder.layers.0.linear2.weight', 'encoder.encoder.layer.3.output.dense.bias', 'encoder.encoder.layer.6.intermediate.dense.weight', 'decoder.layers.3.norm2.weight', 'encoder.encoder.layer.2.attention.self.key.weight', 'encoder.encoder.layer.5.attention.self.value.weight', 'encoder.encoder.layer.7.attention.self.key.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'encoder.encoder.layer.5.attention.self.query.bias', 'decoder.layers.0.self_attn.in_proj_bias', 'encoder.encoder.layer.5.attention.output.dense.weight', 'encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.encoder.layer.8.intermediate.dense.weight', 'encoder.encoder.layer.7.output.dense.weight', 'decoder.layers.4.norm2.bias', 'encoder.encoder.layer.1.attention.self.value.bias', 'encoder.encoder.layer.8.attention.self.key.bias', 'dense.bias', 'encoder.encoder.layer.10.attention.self.key.weight', 'decoder.layers.2.multihead_attn.in_proj_weight', 'encoder.encoder.layer.1.attention.output.dense.bias', 'decoder.layers.3.multihead_attn.out_proj.weight', 'encoder.encoder.layer.3.attention.output.dense.weight', 'encoder.encoder.layer.8.attention.self.value.weight', 'encoder.encoder.layer.6.attention.self.query.weight', 'encoder.encoder.layer.0.attention.self.query.weight', 'encoder.encoder.layer.6.intermediate.dense.bias', 'encoder.encoder.layer.10.intermediate.dense.bias', 'decoder.layers.1.self_attn.in_proj_bias', 'encoder.encoder.layer.3.output.dense.weight', 'encoder.encoder.layer.7.attention.output.dense.weight', 'decoder.layers.4.norm2.weight', 'encoder.encoder.layer.4.attention.self.key.bias', 'decoder.layers.1.norm1.bias', 'encoder.pooler.dense.weight', 'encoder.encoder.layer.4.attention.self.value.bias', 'decoder.layers.5.linear2.weight', 'encoder.encoder.layer.2.output.LayerNorm.weight', 'encoder.encoder.layer.11.attention.output.dense.bias', 'decoder.layers.5.multihead_attn.in_proj_weight', 'decoder.layers.0.self_attn.in_proj_weight', 'decoder.layers.5.norm2.weight', 'encoder.encoder.layer.11.output.LayerNorm.weight', 'encoder.encoder.layer.0.output.LayerNorm.weight', 'encoder.encoder.layer.6.attention.self.key.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at saved_models/dec_6_evosuit_graphcodebert_512_320/methods-tests//checkpoint-best-bleu/pytorch_model.bin and are newly initialized: ['encoder.layer.2.intermediate.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.4.intermediate.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'pooler.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.1.output.dense.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.10.attention.self.value.weight', 'pooler.dense.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.key.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.10.output.LayerNorm.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.9.attention.self.value.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/26/2022 01:19:40 - INFO - __main__ -   reload model from saved_models/dec_6_evosuit_graphcodebert_512_320/methods-tests//checkpoint-best-bleu/pytorch_model.bin
01/26/2022 01:19:43 - INFO - __main__ -   Test file: dataset/evosuit/Evosuit_train.methods,dataset/evosuit/Evosuit_train.tests
  0%|          | 0/13055 [00:00<?, ?it/s]  0%|          | 1/13055 [00:00<50:23,  4.32it/s]  0%|          | 4/13055 [00:00<27:31,  7.90it/s]  0%|          | 5/13055 [00:00<31:34,  6.89it/s]  0%|          | 7/13055 [00:00<25:47,  8.43it/s]  0%|          | 10/13055 [00:01<21:34, 10.08it/s]  0%|          | 12/13055 [00:01<24:42,  8.80it/s]  0%|          | 13/13055 [00:01<26:36,  8.17it/s]  0%|          | 15/13055 [00:01<22:27,  9.68it/s]  0%|          | 17/13055 [00:01<22:54,  9.48it/s]  0%|          | 18/13055 [00:02<25:59,  8.36it/s]  0%|          | 19/13055 [00:02<25:24,  8.55it/s]  0%|          | 22/13055 [00:02<17:53, 12.14it/s]  0%|          | 24/13055 [00:02<24:08,  9.00it/s]  0%|          | 26/13055 [00:02<21:42, 10.00it/s]  0%|          | 28/13055 [00:03<29:48,  7.28it/s]  0%|          | 30/13055 [00:03<25:29,  8.52it/s]  0%|          | 33/13055 [00:03<18:28, 11.75it/s]  0%|          | 35/13055 [00:03<17:36, 12.33it/s]  0%|          | 37/13055 [00:03<19:10, 11.32it/s]  0%|          | 39/13055 [00:04<17:57, 12.08it/s]  0%|          | 41/13055 [00:04<23:50,  9.10it/s]  0%|          | 43/13055 [00:04<25:56,  8.36it/s]  0%|          | 46/13055 [00:04<23:56,  9.05it/s]  0%|          | 50/13055 [00:05<20:30, 10.57it/s]  0%|          | 54/13055 [00:05<17:43, 12.22it/s]  0%|          | 56/13055 [00:05<18:32, 11.69it/s]  0%|          | 58/13055 [00:05<17:37, 12.29it/s]  0%|          | 60/13055 [00:06<23:02,  9.40it/s]  0%|          | 62/13055 [00:06<22:33,  9.60it/s]  0%|          | 64/13055 [00:06<22:25,  9.65it/s]  1%|          | 68/13055 [00:06<16:16, 13.30it/s]  1%|          | 70/13055 [00:07<19:30, 11.09it/s]  1%|          | 72/13055 [00:07<19:46, 10.94it/s]  1%|          | 74/13055 [00:07<21:40,  9.98it/s]  1%|          | 76/13055 [00:07<23:17,  9.28it/s]  1%|          | 77/13055 [00:08<30:04,  7.19it/s]  1%|          | 79/13055 [00:08<31:48,  6.80it/s]  1%|          | 83/13055 [00:08<22:15,  9.71it/s]  1%|          | 87/13055 [00:08<17:59, 12.01it/s]  1%|          | 89/13055 [00:09<22:19,  9.68it/s]  1%|          | 91/13055 [00:09<31:37,  6.83it/s]  1%|          | 93/13055 [00:09<26:47,  8.06it/s]  1%|          | 95/13055 [00:09<23:36,  9.15it/s]  1%|          | 97/13055 [00:10<22:23,  9.64it/s]  1%|          | 99/13055 [00:10<19:16, 11.20it/s]  1%|          | 101/13055 [00:10<23:10,  9.32it/s]  1%|          | 103/13055 [00:10<21:00, 10.28it/s]